{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:12:38.164408Z",
     "start_time": "2025-11-28T07:12:34.598557Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install gensim",
   "id": "519a1279db0064da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\srbuh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\srbuh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\srbuh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\srbuh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\srbuh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:12:49.364236Z",
     "start_time": "2025-11-28T07:12:38.188146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter"
   ],
   "id": "3f4acf7dd1ef6693",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:12:50.589352Z",
     "start_time": "2025-11-28T07:12:50.582936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "BASE_PATH = os.path.join(current_dir, \"ilur-news-corpus\", \"ilur-news-corpus\", \"train\")\n",
    "categories = ['accidents', 'culture', 'economy', 'politics', 'society', 'sport', 'weather']\n",
    "\n",
    "# Models's parameters\n",
    "CONFIG = {\n",
    "    'vector_size': 300,\n",
    "    'window': 5,\n",
    "    'negative': 15,\n",
    "    'epochs': 20,\n",
    "    'min_count': 5,      # Õ°Õ¥Õ¼Õ¡ÖÕ¶Õ¥Õ¬ Õ°Õ¡Õ¦Õ¾Õ¡Õ¤Õ¥Õº Õ¢Õ¡Õ¼Õ¥Ö€Õ¨\n",
    "    'sg': 1,             # Skip-gram architecture\n",
    "    'workers': 4\n",
    "}\n",
    "\n",
    "print(\" Model parameter:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ],
   "id": "410a8a7c5abdc5af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model parameter:\n",
      "   vector_size: 300\n",
      "   window: 5\n",
      "   negative: 15\n",
      "   epochs: 20\n",
      "   min_count: 5\n",
      "   sg: 1\n",
      "   workers: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:12:50.672234Z",
     "start_time": "2025-11-28T07:12:50.666967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Data Preprocessing Functions\n",
    "\n",
    "def clean_armenian_text(text):\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. delete dates (25.11.2024, 25/11/2024)\n",
    "    text = re.sub(r'\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}', '', text)\n",
    "\n",
    "    # 3. delete times (14:30, 14:30:45)\n",
    "    text = re.sub(r'\\d{1,2}:\\d{2}(:\\d{2})?', '', text)\n",
    "\n",
    "    # 4. delete digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    #Unicode: \\u0531-\\u0587\n",
    "    text = re.sub(r'[^\\u0531-\\u0587\\s.!?]', ' ', text)\n",
    "\n",
    "    # 6. Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 7. Trim\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def split_into_sentences(text):\n",
    "\n",
    "    #  . ! ?\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "\n",
    "    words = sentence.split()\n",
    "    return words if len(words) >= 2 else []\n"
   ],
   "id": "a4116ae4ede5a347",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:15:10.019220Z",
     "start_time": "2025-11-28T07:12:50.696470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Load Ö‡ Process All Data\n",
    "\n",
    "def load_and_process_all_data(base_path, categories):\n",
    "\n",
    "    all_sentences = []\n",
    "    all_words = []\n",
    "\n",
    "    stats = {\n",
    "        'files_processed': 0,\n",
    "        'files_failed': 0,\n",
    "        'total_sentences': 0,\n",
    "        'total_words': 0\n",
    "    }\n",
    "    print(\"\\n Reading and processing all data...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        category_files = 0\n",
    "\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"âš   {category}: folder not found\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(category_path):\n",
    "            if not filename.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "\n",
    "            try:\n",
    "                # read file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # text cleaning\n",
    "                cleaned_text = clean_armenian_text(text)\n",
    "\n",
    "                if len(cleaned_text) < 10:  # Skip\n",
    "                    continue\n",
    "\n",
    "                sentences = split_into_sentences(cleaned_text)\n",
    "\n",
    "                # Tokenize sentences\n",
    "                for sentence in sentences:\n",
    "                    words = tokenize_sentence(sentence)\n",
    "                    if words:\n",
    "                        all_sentences.append(words)\n",
    "                        all_words.extend(words)\n",
    "                        stats['total_sentences'] += 1\n",
    "                        stats['total_words'] += len(words)\n",
    "\n",
    "                stats['files_processed'] += 1\n",
    "                category_files += 1\n",
    "\n",
    "                # Progress\n",
    "                if stats['files_processed'] % 100 == 0:\n",
    "                    print(f\"   Processed: {stats['files_processed']} files...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                stats['files_failed'] += 1\n",
    "                print(f\" Error in {filename}: {e}\")\n",
    "\n",
    "        print(f\"âœ“  {category:12s}: {category_files} files\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\" Finish !\\n\")\n",
    "\n",
    "    return all_sentences, all_words, stats\n",
    "\n",
    "# Load data\n",
    "sentences, all_words, stats = load_and_process_all_data(BASE_PATH, categories)\n"
   ],
   "id": "c8645eddf28c7d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reading and processing all data...\n",
      "============================================================\n",
      "   Processed: 100 files...\n",
      "   Processed: 200 files...\n",
      "   Processed: 300 files...\n",
      "   Processed: 400 files...\n",
      "   Processed: 500 files...\n",
      "   Processed: 600 files...\n",
      "   Processed: 700 files...\n",
      "   Processed: 800 files...\n",
      "   Processed: 900 files...\n",
      "   Processed: 1000 files...\n",
      "   Processed: 1100 files...\n",
      "âœ“  accidents   : 1163 files\n",
      "   Processed: 1200 files...\n",
      "   Processed: 1300 files...\n",
      "   Processed: 1400 files...\n",
      "   Processed: 1500 files...\n",
      "   Processed: 1600 files...\n",
      "   Processed: 1700 files...\n",
      "   Processed: 1800 files...\n",
      "   Processed: 1900 files...\n",
      "âœ“  culture     : 796 files\n",
      "   Processed: 2000 files...\n",
      "   Processed: 2100 files...\n",
      "   Processed: 2200 files...\n",
      "   Processed: 2300 files...\n",
      "   Processed: 2400 files...\n",
      "   Processed: 2500 files...\n",
      "   Processed: 2600 files...\n",
      "   Processed: 2700 files...\n",
      "   Processed: 2800 files...\n",
      "   Processed: 2900 files...\n",
      "   Processed: 3000 files...\n",
      "   Processed: 3100 files...\n",
      "   Processed: 3200 files...\n",
      "   Processed: 3300 files...\n",
      "   Processed: 3400 files...\n",
      "   Processed: 3500 files...\n",
      "   Processed: 3600 files...\n",
      "âœ“  economy     : 1653 files\n",
      "   Processed: 3700 files...\n",
      "   Processed: 3800 files...\n",
      "   Processed: 3900 files...\n",
      "   Processed: 4000 files...\n",
      "   Processed: 4100 files...\n",
      "   Processed: 4200 files...\n",
      "   Processed: 4300 files...\n",
      "   Processed: 4400 files...\n",
      "   Processed: 4500 files...\n",
      "   Processed: 4600 files...\n",
      "   Processed: 4700 files...\n",
      "   Processed: 4800 files...\n",
      "   Processed: 4900 files...\n",
      "âœ“  politics    : 1303 files\n",
      "   Processed: 5000 files...\n",
      "   Processed: 5100 files...\n",
      "   Processed: 5200 files...\n",
      "   Processed: 5300 files...\n",
      "   Processed: 5400 files...\n",
      "   Processed: 5500 files...\n",
      "   Processed: 5600 files...\n",
      "   Processed: 5700 files...\n",
      "   Processed: 5800 files...\n",
      "   Processed: 5900 files...\n",
      "   Processed: 6000 files...\n",
      "   Processed: 6100 files...\n",
      "   Processed: 6200 files...\n",
      "âœ“  society     : 1304 files\n",
      "   Processed: 6300 files...\n",
      "   Processed: 6400 files...\n",
      "   Processed: 6500 files...\n",
      "   Processed: 6600 files...\n",
      "   Processed: 6700 files...\n",
      "   Processed: 6800 files...\n",
      "   Processed: 6900 files...\n",
      "   Processed: 7000 files...\n",
      "   Processed: 7100 files...\n",
      "   Processed: 7200 files...\n",
      "   Processed: 7300 files...\n",
      "   Processed: 7400 files...\n",
      "   Processed: 7500 files...\n",
      "   Processed: 7600 files...\n",
      "   Processed: 7700 files...\n",
      "   Processed: 7800 files...\n",
      "   Processed: 7900 files...\n",
      "   Processed: 8000 files...\n",
      "   Processed: 8100 files...\n",
      "   Processed: 8200 files...\n",
      "   Processed: 8300 files...\n",
      "   Processed: 8400 files...\n",
      "âœ“  sport       : 2238 files\n",
      "   Processed: 8500 files...\n",
      "   Processed: 8600 files...\n",
      "   Processed: 8700 files...\n",
      "   Processed: 8800 files...\n",
      "   Processed: 8900 files...\n",
      "   Processed: 9000 files...\n",
      "   Processed: 9100 files...\n",
      "   Processed: 9200 files...\n",
      "   Processed: 9300 files...\n",
      "   Processed: 9400 files...\n",
      "   Processed: 9500 files...\n",
      "   Processed: 9600 files...\n",
      "   Processed: 9700 files...\n",
      "   Processed: 9800 files...\n",
      "   Processed: 9900 files...\n",
      "âœ“  weather     : 1484 files\n",
      "============================================================\n",
      " Finish !\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:15:10.319914Z",
     "start_time": "2025-11-28T07:15:10.077721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"ÕŽÔ»ÕƒÔ±Ô¿Ô±Ô³ÕÕˆÕ’Ô¹Õ…ÕˆÕ’Õ†:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Õ–Õ¡ÕµÕ¬Õ¥Ö€ (processed):      {stats['files_processed']}\")\n",
    "print(f\"   Õ–Õ¡ÕµÕ¬Õ¥Ö€ (failed):         {stats['files_failed']}\")\n",
    "print(f\"   Õ†Õ¡Õ­Õ¡Õ¤Õ¡Õ½Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€:       {stats['total_sentences']:,}\")\n",
    "print(f\"   Ô¸Õ¶Õ¤Õ¡Õ´Õ¥Õ¶Õ¨ Õ¢Õ¡Õ¼Õ¥Ö€:         {stats['total_words']:,}\")\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "unique_words = len(word_freq)\n",
    "\n",
    "min_count = CONFIG['min_count']\n",
    "rare_words = sum(1 for count in word_freq.values() if count < min_count)\n",
    "kept_words = unique_words - rare_words\n",
    "\n",
    "print(f\"\\n   Unique Õ¢Õ¡Õ¼Õ¥Ö€:           {unique_words:,}\")\n",
    "print(f\"   Õ€Õ¡Õ¦Õ¾Õ¡Õ¤Õ¥Õº Õ¢Õ¡Õ¼Õ¥Ö€ (< {min_count}):  {rare_words:,}\")\n",
    "print(f\"   Ô¿ÕºÕ¡Õ°Õ¾Õ¸Õ² Õ¢Õ¡Õ¼Õ¥Ö€:          {kept_words:,}\")\n",
    "\n",
    "print(f\"\\n Ô±Õ´Õ¥Õ¶Õ¡Õ°Õ¡Õ³Õ¡Õ­ 15 Õ¢Õ¡Õ¼Õ¥Ö€Õ¨:\")\n",
    "for word, count in word_freq.most_common(15):\n",
    "    print(f\"   {word:15s}: {count:6,}\")\n",
    "\n",
    "print(\"=\"*60)"
   ],
   "id": "40c1dfbc1231cbd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÕŽÔ»ÕƒÔ±Ô¿Ô±Ô³ÕÕˆÕ’Ô¹Õ…ÕˆÕ’Õ†:\n",
      "============================================================\n",
      "   Õ–Õ¡ÕµÕ¬Õ¥Ö€ (processed):      9941\n",
      "   Õ–Õ¡ÕµÕ¬Õ¥Ö€ (failed):         0\n",
      "   Õ†Õ¡Õ­Õ¡Õ¤Õ¡Õ½Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶Õ¶Õ¥Ö€:       34,793\n",
      "   Ô¸Õ¶Õ¤Õ¡Õ´Õ¥Õ¶Õ¨ Õ¢Õ¡Õ¼Õ¥Ö€:         1,389,413\n",
      "\n",
      "   Unique Õ¢Õ¡Õ¼Õ¥Ö€:           94,983\n",
      "   Õ€Õ¡Õ¦Õ¾Õ¡Õ¤Õ¥Õº Õ¢Õ¡Õ¼Õ¥Ö€ (< 5):  72,356\n",
      "   Ô¿ÕºÕ¡Õ°Õ¾Õ¸Õ² Õ¢Õ¡Õ¼Õ¥Ö€:          22,627\n",
      "\n",
      " Ô±Õ´Õ¥Õ¶Õ¡Õ°Õ¡Õ³Õ¡Õ­ 15 Õ¢Õ¡Õ¼Õ¥Ö€Õ¨:\n",
      "   Õ§              : 63,859\n",
      "   Ö‡              : 21,432\n",
      "   Õ¸Ö€             : 18,232\n",
      "   Õ¥Õ¶             : 17,826\n",
      "   Õ«              : 11,745\n",
      "   Õ«Õ¶             : 10,964\n",
      "   Õ¥Ö‚             :  8,073\n",
      "   Õ§Ö€             :  6,483\n",
      "   Õ°Õ¡Õ´Õ¡Ö€          :  6,223\n",
      "   Õ¸Ö‚             :  5,909\n",
      "   Õ¡ÕµÕ½            :  5,545\n",
      "   Õ°Õ¥Õ¿            :  5,499\n",
      "   Õ¡ÕµÕ¤            :  5,144\n",
      "   Õ°Õ°             :  4,717\n",
      "   Õ«Õ½Õ¯            :  4,711\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:18:36.139054Z",
     "start_time": "2025-11-28T07:15:10.329565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Word2Vec Model\n",
    "\n",
    "print(\"\\n training Word2Vec model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=CONFIG['vector_size'],\n",
    "    window=CONFIG['window'],\n",
    "    min_count=CONFIG['min_count'],\n",
    "    workers=CONFIG['workers'],\n",
    "    sg=CONFIG['sg'],\n",
    "    negative=CONFIG['negative'],\n",
    "    epochs=CONFIG['epochs']\n",
    ")\n",
    "\n",
    "print(\" Training is finished\")\n",
    "print(f\"   Vocabulary size: {len(model.wv):,} word\")\n",
    "print(\"=\"*60)"
   ],
   "id": "ff86feaf1db1489d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training Word2Vec model...\n",
      "============================================================\n",
      " Training is finished\n",
      "   Vocabulary size: 22,627 word\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:18:36.930777Z",
     "start_time": "2025-11-28T07:18:36.695716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save Model\n",
    "\n",
    "model.save(\"armenian_word2vec.model\")\n",
    "model.wv.save(\"armenian_word_vectors.kv\")\n",
    "print(f\"\\n Saved:\")\n",
    "print(f\"   - armenian_word2vec.model\")\n",
    "print(f\"   - armenian_word_vectors.kv\")\n"
   ],
   "id": "e37829dd6c43f243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saved:\n",
      "   - armenian_word2vec.model\n",
      "   - armenian_word_vectors.kv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T07:18:37.021793Z",
     "start_time": "2025-11-28T07:18:36.949404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "print(\"\\nðŸ” Model Evaluation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Similar words:\\n\")\n",
    "\n",
    "test_words = ['Õ¥Ö€Ö‡Õ¡Õ¶', 'Õ°Õ¡ÕµÕ¡Õ½Õ¿Õ¡Õ¶', 'Õ¯Õ¡Õ¼Õ¡Õ¾Õ¡Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶', 'Õ½ÕºÕ¸Ö€Õ¿', 'Õ¥Õ²Õ¡Õ¶Õ¡Õ¯']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=5)\n",
    "        print(f\"   '{word}' â†’\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"      {sim_word:20s} ({score:.3f})\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"   âš ï¸ '{word}' not found in vocabulary\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… \")"
   ],
   "id": "27bfdde8509d547e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Model Evaluation:\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ Similar words:\n",
      "\n",
      "   'Õ¥Ö€Ö‡Õ¡Õ¶' â†’\n",
      "      Ö„Õ¡Õ²Õ¡Ö„Õ¸Ö‚Õ´Õ            (0.528)\n",
      "      Õ¥Ö€Ö‡Õ¡Õ¶Õ¸Ö‚Õ´Õ            (0.514)\n",
      "      Õ¥Ö€Ö‡Õ¡Õ¶Õ¸Ö‚Õ´ÕÕ´Õ¡Ö€Õ¿Õ«       (0.510)\n",
      "      Ö„Õ¡Õ²Õ¡Ö„Õ¸Ö‚Õ´ÕÕ¤Õ¥Õ¯Õ¿Õ¥Õ´Õ¢Õ¥Ö€Õ«  (0.506)\n",
      "      Ö„Õ¡Õ²Õ¡Ö„Õ«               (0.506)\n",
      "\n",
      "   'Õ°Õ¡ÕµÕ¡Õ½Õ¿Õ¡Õ¶' â†’\n",
      "      Õ¢Õ¡Ö€Õ£Õ¡Õ¾Õ¡Õ³             (0.457)\n",
      "      Õ«Ö€Õ¡Õ¶                 (0.446)\n",
      "      Õ¬ÕµÕ¸Ö‚Ö„Õ½Õ¥Õ´Õ¢Õ¸Ö‚Ö€Õ£        (0.436)\n",
      "      Õ¾Ö€Õ¡Õ½Õ¿Õ¡Õ¶              (0.435)\n",
      "      Õ°Õ¡Õ¶Ö€Õ¡ÕºÕ¥Õ¿Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶      (0.433)\n",
      "\n",
      "   'Õ¯Õ¡Õ¼Õ¡Õ¾Õ¡Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶' â†’\n",
      "      Õ¡Ö€Õ¾Õ¥Õ¬Õ¸Ö‚              (0.457)\n",
      "      Õ«Õ·Õ­Õ¡Õ¶Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶         (0.437)\n",
      "      Õ°Õ¥Õ¼Õ¡Õ¶Õ¯Õ¡Ö€             (0.436)\n",
      "      Õ°Õ¡Õ½Õ¡Ö€Õ¡Õ¯Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶       (0.423)\n",
      "      Õ°Õ¡Ö€Õ½Õ¿Õ¡Õ¶Õ¸Ö‚Õ´           (0.420)\n",
      "\n",
      "   'Õ½ÕºÕ¸Ö€Õ¿' â†’\n",
      "      Õ§Ö„Õ½ÕºÖ€Õ¥Õ½              (0.638)\n",
      "      Õ¤Õ¡Õ°Õ¸Ö‚Õ¯Õ¡ÕµÕ«Õ¶           (0.526)\n",
      "      Õ¡Õ©Õ¬Õ¥Õ¿Õ«Õ¯Õ¡             (0.492)\n",
      "      Õ·Õ¡Õ¢Õ¡Õ©Õ¡Õ©Õ¥Ö€Õ©Õ«          (0.455)\n",
      "      Õ½Õ¸Ö‚ÕºÕ¥Ö€Õ´Õ«Õ»Õ«Õ¶          (0.450)\n",
      "\n",
      "   'Õ¥Õ²Õ¡Õ¶Õ¡Õ¯' â†’\n",
      "      Õ¿Õ¥Õ²Õ¸Ö‚Õ´Õ¶Õ¥Ö€Õ«           (0.761)\n",
      "      Õ¡Õ¶Õ±Ö€Ö‡                (0.673)\n",
      "      Õ½ÕºÕ¡Õ½Õ¾Õ¸Ö‚Õ´             (0.647)\n",
      "      Õ¡Õ´ÕºÖ€Õ¸Õº               (0.628)\n",
      "      Õ´Õ¡Õ¼Õ¡Õ­Õ¸Ö‚Õ²Õ«Ö           (0.619)\n",
      "\n",
      "============================================================\n",
      "âœ… \n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
